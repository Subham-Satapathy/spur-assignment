# AI-Powered Customer Support Chat

An intelligent, production-ready customer support chatbot built with a **modular monolith architecture**. Features multi-channel support readiness, tool/function calling infrastructure, and extensible LLM provider integration.

üîó **Live Demo**: [https://spur-shop.subhs.xyz/](https://spur-shop.subhs.xyz/)

---

## Quick Start

### Prerequisites

- **Node.js** 18+ and npm
- **PostgreSQL** database (NeonDB, Railway, or local)
- **LLM API Key**: 
  - **OpenAI API Key** (for GPT-4/GPT-3.5) OR
  - **OpenRouter API Key** (for access to multiple models including free options)
- Optional: **Redis** (for caching)

---

## Installation & Setup

### 1. Clone and Install Dependencies

```bash
# Clone repository
cd "Spur Assignment"

# Install backend dependencies
npm install

# Install frontend dependencies
cd frontend
npm install
cd ..
```

### 2. Database Setup

#### Option A: Using NeonDB (Recommended for Production)

1. Create a free account at [neon.tech](https://neon.tech)
2. Create a new project and database
3. Copy the connection string (it looks like `postgresql://user:pass@host.neon.tech/dbname`)

#### Option B: Local PostgreSQL

```bash
# Install PostgreSQL (macOS)
brew install postgresql@15
brew services start postgresql@15

# Create database
createdb ai_chat_support
```

### 3. Environment Configuration

Create a `.env` file in the project root:

```bash
# Copy example env file
cp .env.example .env
```

Edit `.env` with your configuration:

```env
# Server
PORT=3000
NODE_ENV=development

# Database (use your NeonDB or local PostgreSQL URL)
DATABASE_URL=postgresql://user:password@host:5432/dbname?sslmode=require

# LLM Provider Configuration
# Choose one: "openai", "openrouter", or "anthropic"
LLM_PROVIDER=openrouter

# Primary LLM API Configuration (REQUIRED)
LLM_API_KEY=your-api-key-here
LLM_MODEL=openai/gpt-oss-20b:free

# OpenAI Configuration (when using LLM_PROVIDER=openai)
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_MODEL=gpt-4

# OpenRouter Configuration (when using LLM_PROVIDER=openrouter)
# Get your API key from: https://openrouter.ai/keys
# Browse models at: https://openrouter.ai/models
# Example models:
#   - openai/gpt-4-turbo
#   - openai/gpt-3.5-turbo
#   - openai/gpt-oss-20b:free (free model, no credit card required)
#   - anthropic/claude-3-opus
#   - meta-llama/llama-3-70b-instruct
OPENROUTER_REFERER=https://yourdomain.com
OPENROUTER_TITLE=Customer Support Agent

# Optional: Advanced LLM Configuration
LLM_MAX_TOKENS=500
LLM_TEMPERATURE=0.7

# Optional: Redis (for production caching)
REDIS_ENABLED=false
REDIS_URL=redis://localhost:6379

# Application Settings
MAX_MESSAGE_LENGTH=2000
MAX_CONVERSATION_HISTORY=10

# Feature Flags (disabled by default - architecture in place)
ENABLE_TOOLS=false
TELEGRAM_ENABLED=false
WHATSAPP_ENABLED=false
```

### 4. Database Migration & Seeding

```bash
# Push schema to database (creates tables)
npm run db:push

# Seed with sample knowledge base data
npm run db:seed

# Or do both in one command
npm run db:setup
```

This will create:
- `conversations` table
- `messages` table
- `knowledge_entries` table
- `users` table (for future multi-channel support)
- `tool_executions` table (for future AI agent tools)

Sample knowledge base includes:
- Shipping policies
- Return & refund process
- Payment methods
- Support hours & contact info

### 5. Run the Application

#### Development Mode (with hot reload)

```bash
# Run both backend and frontend
npm run dev:all

# Or run separately
npm run dev              # Backend only (port 3000)
npm run dev:frontend     # Frontend only (port 5173)
```

#### Production Mode

```bash
# Build everything
npm run build:all

# Start server
npm start
```

**Open your browser**: http://localhost:5173 (dev) or http://localhost:3000 (production)

---

## Architecture Overview

### **Modular Monolith Structure**

The backend follows a **feature-based modular monolith** architecture with clear separation of concerns:

```
src/
‚îú‚îÄ‚îÄ modules/              # Feature modules (independently testable)
‚îÇ   ‚îú‚îÄ‚îÄ api/             # HTTP layer (Express routes, middleware)
‚îÇ   ‚îú‚îÄ‚îÄ chat/            # Chat orchestration service
‚îÇ   ‚îú‚îÄ‚îÄ conversation/    # Conversation & message persistence
‚îÇ   ‚îú‚îÄ‚îÄ knowledge/       # Knowledge base management
‚îÇ   ‚îú‚îÄ‚îÄ llm/             # LLM provider abstraction
‚îÇ   ‚îú‚îÄ‚îÄ messaging/       # Event bus (pub/sub)
‚îÇ   ‚îú‚îÄ‚îÄ channels/        # Multi-channel support (WEB, Telegram, WhatsApp)
‚îÇ   ‚îî‚îÄ‚îÄ tools/           # AI function calling tools
‚îú‚îÄ‚îÄ shared/              # Cross-cutting concerns
‚îÇ   ‚îú‚îÄ‚îÄ config/         # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database/       # Database client (Drizzle ORM)
‚îÇ   ‚îú‚îÄ‚îÄ errors/         # Custom error classes
‚îÇ   ‚îú‚îÄ‚îÄ logger/         # Winston logging
‚îÇ   ‚îú‚îÄ‚îÄ redis/          # Redis client (caching)
‚îÇ   ‚îî‚îÄ‚îÄ types/          # Shared TypeScript types
‚îî‚îÄ‚îÄ scripts/            # Utility scripts (migration, seed)
```

### **Layered Architecture**

1. **API Layer** (`modules/api`)
   - Express routes, middleware
   - Request validation (Zod)
   - Rate limiting, error handling

2. **Service Layer** (`modules/*/service.ts`)
   - Business logic
   - Orchestration between modules
   - Transaction management

3. **Repository Layer** (`modules/*/repository.ts`)
   - Database access (Drizzle ORM)
   - Query optimization
   - Data mapping

4. **Domain Layer** (`shared/types`)
   - Type definitions
   - Business entities
   - Validation rules

### **Key Design Decisions**

#### **Interface-Based LLM Provider**
```typescript
interface ILLMProvider {
  generateReply(context: ConversationContext, tools?: ToolDefinition[]): Promise<LLMResponse>;
  healthCheck(): Promise<boolean>;
}
```
- **Why**: Easy to swap OpenAI ‚Üí Anthropic ‚Üí Local models
- **Factory Pattern**: `LLMProviderFactory` creates providers from config
- **Benefit**: Zero code changes to switch AI providers

#### **Channel Abstraction for Multi-Platform**
```typescript
interface IChannel {
  type: ChannelType;
  sendMessage(message: OutgoingMessage): Promise<void>;
  parseIncomingMessage(payload: any): IncomingMessage | null;
  verifyWebhook(payload: any, signature?: string): boolean;
}
```
- **Why**: Designed for Telegram, WhatsApp, Instagram integration
- **Current**: Web channel fully functional
- **Future**: Enable other channels via env vars + API keys
- **Benefit**: Each platform isolated, no cross-contamination

#### **Tool Registry for AI Agent Capabilities**
```typescript
class ToolRegistry {
  register(tool: ITool): void;
  execute(name: string, params: any): Promise<any>;
  getAllDefinitions(): ToolDefinition[];
}
```
- **Why**: Enables AI to call functions (check inventory, track orders, etc.)
- **Architecture**: Ready but disabled by default (`ENABLE_TOOLS=false`)
- **Benefit**: Add Shopify/Stripe integrations without touching core logic

#### **Event-Driven Communication**
```typescript
eventBus.publish({ type: 'MESSAGE_RECEIVED', payload: { ... } });
eventBus.subscribe('MESSAGE_RECEIVED', handler);
```
- **Why**: Decouples modules (analytics, webhooks, notifications)
- **Benefit**: Add features without modifying existing code

#### **Two-Tier Caching Strategy**
- **In-Memory**: Fast, per-instance (knowledge base)
- **Redis**: Shared across instances, persistent
- **Graceful Degradation**: Falls back to in-memory if Redis unavailable

---

## LLM Integration

### **Supported Providers**

#### **OpenRouter (Recommended for Flexibility)**

OpenRouter provides access to multiple LLM providers through a unified API.

**Why OpenRouter?**
- Access to 100+ models from multiple providers (OpenAI, Anthropic, Meta, Google, etc.)
- Free tier available with models like `openai/gpt-oss-20b:free`
- No credit card required for free models
- Unified API for all providers
- Automatic fallback and load balancing
- Pay-per-use pricing (only pay for what you use)

**Getting Started with OpenRouter:**
1. Sign up at [openrouter.ai](https://openrouter.ai)
2. Get your API key from [openrouter.ai/keys](https://openrouter.ai/keys)
3. Browse available models at [openrouter.ai/models](https://openrouter.ai/models)
4. Set `LLM_PROVIDER=openrouter` in your `.env`

**Recommended Models:**
- `openai/gpt-oss-20b:free` - Free, no credit card required
- `openai/gpt-3.5-turbo` - Fast and cost-effective
- `openai/gpt-4-turbo` - Best quality
- `anthropic/claude-3-sonnet` - Excellent for conversations
- `meta-llama/llama-3-70b-instruct` - Open-source alternative

#### **OpenAI (Direct)**

Direct integration with OpenAI's API using GPT-4 or GPT-3.5-turbo.

**Why OpenAI Direct?**
- Best-in-class function calling support
- Reliable API with good rate limits
- Excellent instruction following
- Strong reasoning for customer support scenarios

**Getting Started with OpenAI:**
1. Sign up at [platform.openai.com](https://platform.openai.com)
2. Get your API key from the dashboard
3. Set `LLM_PROVIDER=openai` in your `.env`

#### **Anthropic Claude (Coming Soon)**

Support for Anthropic's Claude models is planned.

### **Provider Configuration**

Switch between providers by changing a single environment variable:

```env
# Use OpenRouter
LLM_PROVIDER=openrouter
LLM_API_KEY=sk-or-v1-...
LLM_MODEL=openai/gpt-oss-20b:free

# OR use OpenAI directly
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4
```

### **Prompting Strategy**

#### System Prompt Structure
```
You are a helpful and friendly customer support agent for an e-commerce store.

Guidelines:
- Be polite, empathetic, and helpful
- Provide accurate information based on the knowledge base
- If you don't know something, admit it
- Keep responses concise but complete
- Use a warm, conversational tone

[Knowledge Base Content]
## Shipping
...
## Returns & Refunds
...
```

#### Context Management
- **Conversation History**: Last 10 messages (configurable)
- **Knowledge Base**: Cached and injected into system prompt
- **Token Optimization**: 
  - Max tokens: 500 (prevents overly verbose responses)
  - Temperature: 0.7 (balanced creativity/consistency)

#### Function Calling (Architecture Ready)
```typescript
// Tools available to LLM (when enabled)
{
  "name": "check_inventory",
  "description": "Check product stock levels",
  "parameters": { "productId": "string" }
}
```

### **LLM Configuration**

Easily customizable via environment variables:

```env
LLM_PROVIDER=openrouter       # Provider: openai, openrouter, or anthropic
LLM_MODEL=openai/gpt-oss-20b:free  # Model name (provider-specific)
LLM_MAX_TOKENS=500            # Response length limit
LLM_TEMPERATURE=0.7           # Creativity (0-1)
```

**Model Selection Tips:**
- For production: Use GPT-4 or Claude for best quality
- For development: Use GPT-3.5 or free models to save costs
- For high volume: Consider cheaper models with caching
- For privacy: Use local models (future feature)

### **Error Handling**

Comprehensive error mapping for better UX:
- Rate limits ‚Üí "AI service temporarily busy"
- Context length ‚Üí "Conversation too long, start new one"
- Network errors ‚Üí "Cannot connect, try again"
- Invalid API key ‚Üí "Configuration error, contact support"

---

## API Documentation

### **Swagger UI (Interactive API Docs)**

Access the complete API documentation with Swagger UI:

**Development**: http://localhost:3000/api-docs  
**Production**: https://spur-shop.subhs.xyz/api-docs

#### Features:
- üöÄ **Interactive Testing**: Test endpoints directly from the browser
- üìã **Request/Response Examples**: View sample payloads and responses
- üìä **Schema Definitions**: Detailed information about data models
- üîê **Security Documentation**: Authentication and webhook verification
- ‚ö†Ô∏è **Error Responses**: Documented error codes and messages

### API Endpoints

### Chat API

```bash
# Send a message
POST /chat/message
Content-Type: application/json

{
  "message": "What are your shipping policies?",
  "sessionId": "optional-conversation-id"
}

Response:
{
  "reply": "We offer free shipping on orders over $50...",
  "sessionId": "uuid-of-conversation",
  "processingTime": 1234
}
```

```bash
# Get conversation history
GET /chat/conversation/:sessionId

Response:
{
  "sessionId": "uuid",
  "messages": [
    {
      "id": "msg-uuid",
      "sender": "user",
      "text": "Hello",
      "timestamp": "2024-01-20T10:00:00Z"
    }
  ]
}
```

### Health Check

```bash
GET /health

Response:
{
  "status": "healthy",
  "timestamp": "2024-01-20T10:00:00Z",
  "database": "connected",
  "llm": "healthy"
}
```

### Webhook Endpoints

```bash
# Telegram webhook (ready for integration)
POST /webhooks/telegram

# WhatsApp webhook (ready for integration)
POST /webhooks/whatsapp
GET /webhooks/whatsapp  # Verification endpoint
```

#### Adding Documentation to New Endpoints

Use JSDoc comments with the `@swagger` tag:

```typescript
/**
 * @swagger
 * /your-endpoint:
 *   get:
 *     summary: Brief description
 *     description: Detailed description
 *     tags: [YourTag]
 *     responses:
 *       200:
 *         description: Success response
 */
router.get('/your-endpoint', handler);
```

---

## Trade-offs & "If I Had More Time..."

### **Current Trade-offs**

#### **Chose: Modular Monolith**
- **Instead of**: Microservices
- **Why**: Faster development, easier deployment, suitable for MVP
- **Trade-off**: Single deployment unit (vs independent scaling)

#### **Chose: PostgreSQL with Drizzle ORM**
- **Instead of**: MongoDB, Prisma
- **Why**: Type-safe SQL, excellent performance, migrations
- **Trade-off**: More verbose than document stores

#### **Chose: OpenAI Only**
- **Instead of**: Multi-provider from day 1
- **Why**: Focus on UX and architecture first
- **Trade-off**: Vendor lock-in (mitigated by abstraction layer)

#### **Chose: Single Web Channel**
- **Instead of**: All channels at once
- **Why**: Prove architecture extensibility without overbuilding
- **Trade-off**: Telegram/WhatsApp placeholder implementations

### **If I Had More Time... (Priority Order)**

#### **Priority 1: Production Hardening**
- [ ] **Comprehensive testing**
  - Unit tests for services (Jest)
  - Integration tests for API endpoints (Supertest)
  - E2E tests for frontend (Playwright)
  - Target: 80%+ coverage

- [ ] **Observability**
  - OpenTelemetry tracing
  - Prometheus metrics
  - Grafana dashboards
  - Error tracking (Sentry)

- [ ] **Security enhancements**
  - JWT authentication
  - API key management
  - Input sanitization
  - Rate limiting per user (not just IP)

#### **Priority 2: Advanced Features**
- [ ] **Real AI Agent Tools**
  - Shopify API integration (inventory, orders)
  - Stripe payment links
  - Email/SMS notifications
  - Calendar booking

- [ ] **Multi-Channel Support**
  - Telegram Bot API (webhook ready, implementation pending)
  - WhatsApp Business API (webhook ready, implementation pending)
  - Instagram Messaging
  - Facebook Messenger

- [ ] **Conversation Intelligence**
  - Sentiment analysis
  - Intent classification
  - Auto-escalation to human agents
  - CSAT surveys

#### **Priority 3: Scale & Optimization**
- [ ] **Performance**
  - WebSocket for real-time updates
  - GraphQL for flexible queries
  - CDN for static assets
  - Database query optimization

- [ ] **Analytics Dashboard**
  - Conversation metrics
  - Response time tracking
  - User satisfaction scores
  - Knowledge base gap analysis

- [ ] **Multi-tenancy**
  - Support multiple stores
  - Per-tenant knowledge bases
  - Isolated data and configs

#### **Nice-to-Have Enhancements**
- [ ] Voice input/output
- [ ] File upload support (images, receipts)
- [ ] Multilingual support
- [ ] Admin panel for knowledge base management
- [ ] A/B testing for prompts
- [ ] Local LLM support (Ollama, LLaMA)

---

## Technology Stack

### **Backend**
- **Runtime**: Node.js 18+ with TypeScript
- **Framework**: Express.js
- **Database**: PostgreSQL (NeonDB recommended)
- **ORM**: Drizzle ORM
- **Validation**: Zod
- **Logging**: Winston
- **Caching**: Redis (optional, with in-memory fallback)

### **Frontend**
- **Framework**: Svelte + Vite
- **Styling**: Custom CSS (no frameworks)
- **State**: Svelte stores
- **Build**: Vite (fast HMR)

### **AI/LLM**
- **Provider**: OpenAI (GPT-4 / GPT-3.5-turbo)
- **Abstraction**: Custom `ILLMProvider` interface
- **Tool Calling**: OpenAI function calling API

### **DevOps**
- **Deployment**: Render, Railway, or Vercel
- **CI/CD**: GitHub Actions (ready to add)
- **Monitoring**: Winston logs (ready for aggregation)

---

## Development Commands

```bash
# Development
npm run dev              # Backend with hot reload
npm run dev:frontend     # Frontend with hot reload
npm run dev:all          # Both concurrently

# Building
npm run build            # Compile TypeScript
npm run build:frontend   # Build frontend assets
npm run build:all        # Build everything

# Database
npm run db:generate      # Generate migrations from schema
npm run db:push          # Push schema to database
npm run db:migrate       # Run migrations
npm run db:seed          # Seed knowledge base
npm run db:setup         # Push + Seed (first-time setup)

# Code Quality
npm run lint             # ESLint check
npm run type-check       # TypeScript check (no emit)
```

---

## Project Structure

```
.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app.ts                    # Application bootstrap
‚îÇ   ‚îú‚îÄ‚îÄ server.ts                 # HTTP server entry point
‚îÇ   ‚îú‚îÄ‚îÄ modules/                  # Feature modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/                # Chat orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation/        # Message persistence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ knowledge/           # Knowledge base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/                 # LLM abstraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ channels/            # Multi-channel (WEB, Telegram, WhatsApp)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools/               # AI function calling tools
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ messaging/           # Event bus
‚îÇ   ‚îú‚îÄ‚îÄ shared/                   # Shared utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/            # Database client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ errors/              # Error classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger/              # Logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis/               # Redis client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types/               # TypeScript types
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                  # Utility scripts
‚îú‚îÄ‚îÄ frontend/                     # Svelte frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.svelte           # Main app component
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/          # UI components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.js            # Frontend config
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.js           # Vite configuration
‚îú‚îÄ‚îÄ migrations/                   # SQL migrations
‚îú‚îÄ‚îÄ drizzle.config.ts            # Drizzle ORM config
‚îú‚îÄ‚îÄ package.json                 # Dependencies
‚îú‚îÄ‚îÄ tsconfig.json                # TypeScript config
‚îî‚îÄ‚îÄ .env.example                 # Example environment vars
```

---

## Deployment

### Render (Recommended)

1. **Create PostgreSQL database** (or use NeonDB)
2. **Create Web Service**
   - Build command: `npm run build:all`
   - Start command: `npm start`
3. **Environment variables**: Copy from `.env.example`
4. **Deploy**: Push to GitHub and connect

### Railway

```bash
railway init
railway add --database postgresql
railway up
```

### Docker (Coming Soon)

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --production
COPY . .
RUN npm run build:all
CMD ["npm", "start"]
```

---

## Contributing

This is a technical assessment project, but feedback is welcome!

### Future Contributions Could Include:
- Real channel implementations (Telegram, WhatsApp)
- Additional AI tools (Shopify, Stripe, etc.)
- Testing suite
- Internationalization (i18n)

---

## License

MIT License - Feel free to use for learning and reference.

---

## Acknowledgments

Built as a technical assessment demonstrating:
- Clean architecture principles
- Extensible design patterns
- Production-ready code quality
- Modern TypeScript best practices
- Thoughtful AI/LLM integration

---

**Made with ‚ù§Ô∏è by Subham**
